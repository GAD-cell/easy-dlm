
# A2D Training Configuration - Minimal

# Model
model_name: "MaxLSB/LeCarnet-3M"
tokenizer_name: "MaxLSB/LeCarnet-3M"

# Dataset
dataset_name: "LeCarnet"
dataset_path: "MaxLSB/LeCarnet"
dataset_config_name: 
text_column: "text"

# Output
output_dir: "./a2d_output"
cache_dir: "./cache"


#LoRA config
use_lora: false
lora_r: 8
lora_alpha: 16
lora_dropout: 0.1
lora_target_modules: ["q_proj", "v_proj", "o_proj", "k_proj", "up_proj", "down_proj"]


# Training
num_train_epochs: 3
per_device_train_batch_size: 40
gradient_accumulation_steps: 2
learning_rate: 5.0e-4
lr_muon: 3.0e-4
lr_adam: 1.0e-8
lr_scheduler_type: "cosine"
warmup_steps: 1000
max_grad_norm: 1.0
weight_decay: 0.1

# Data
block_size: 512

# Logging
eval_steps: 200
save_steps: 1000
logging_steps: 5

# Performance
bf16: true